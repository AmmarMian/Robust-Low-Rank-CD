##############################################################################
# Functions used to detect a change in the parameters of a SIRV distribution
# Authored by Ammar Mian, 28/09/2018
# e-mail: ammar.mian@centralesupelec.fr
##############################################################################
# Copyright 2018 @CentraleSupelec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
import numpy as np
import scipy as sp
import warnings
from generic_functions import *


##############################################################################
# Gaussian Statistics
##############################################################################
def covariance_equality_glrt_gaussian_statistic(ğ—, args):
    """ GLRT statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = sclae
        Outputs:
            * the GLRT statistic given the observations in input"""

    (p, N, T) = ğ—.shape
    S = 0
    logDenominator = 0
    for t in range(0, T):
        St = SCM(ğ—[:, :, t])
        logDenominator = logDenominator + N * np.log(np.abs(np.linalg.det(St)))
        S = S + St / T
    logNumerator = N * T * np.log(np.abs(np.linalg.det(S)))
    if args is not None:
        if args=='log':
            return np.real(logNumerator - logDenominator)
        else:
            return np.exp(np.real(logNumerator - logDenominator))
    return np.exp(np.real(logNumerator - logDenominator))


def covariance_equality_glrt_gaussian_statistic_low_rank(ğ—, args):
    """ Low rank GLRT statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = rank and scale
        Outputs:
            * the GLRT statistic given the observations in input"""

    (p, N, T) = ğ—.shape
    R, scale = args
    S = 0

    # Estimating Ïƒ2 
    ğšº = SCM(X.reshape((p,T*N)))
    u, s, vh = np.linalg.svd(ğšº)
    Ïƒ2 = s[R:].mean()

    # Estimating S using all observations
    S = SCM(X.reshape((p,N*T)))
    # Imposing low rank structure
    u, s, vh = np.linalg.svd(S)
    s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
    s_noise = Ïƒ2 * np.ones((p-R,))
    s = np.diag(np.hstack([s_signal, s_noise]))
    S = u @ s @ u.conj().T

    logNumerator = N * T * np.log(np.abs(np.linalg.det(S)))
    logDenominator = 0
    for t in range(0, T):
        St = SCM(ğ—[:, :, t])
        # Imposing low rank structure
        u, s, vh = np.linalg.svd(St)
        s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
        s_noise = Ïƒ2 * np.ones((p-R,))
        s = np.diag(np.hstack([s_signal, s_noise]))
        St = u @ s @ vh

        logDenominator = logDenominator + N * np.log(np.abs(np.linalg.det(St)))
  

    if scale=='log':
        return np.real(logNumerator - logDenominator)
    else:
        return np.exp(np.real(logNumerator - logDenominator))

    # # Other implementation
    # iS = np.linalg.inv(S)
    # log_likelihood_H0 = np.real(- N * T * np.log(np.abs(np.linalg.det(S))) )
    # log_likelihood_H1 = 0
    # for t in range(0, T):

    #     # Computing log_likelihood under H0
    #     log_likelihood_H0 = log_likelihood_H0 - np.real( np.trace((ğ—[:,:,t]@ğ—[:,:,t].conj().T)@iS) )

    #     # Computing log_likelihood under H1

    #     St = SCM(ğ—[:, :, t])
    #     # Imposing low rank structure
    #     u, s, vh = np.linalg.svd(St)
    #     s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
    #     s_noise = Ïƒ2 * np.ones((p-R,))
    #     s = np.diag(np.hstack([s_signal, s_noise]))
    #     St = u @ s @ u.conj().T

    #     log_likelihood_H1 = log_likelihood_H1 - np.real(N * np.log(np.abs(np.linalg.det(St)))) - \
    #                         np.real( np.trace((ğ—[:,:,t]@ğ—[:,:,t].conj().T)@np.linalg.inv(St)) )
   
    # if scale=='log':
    #     return np.real(log_likelihood_H1 - log_likelihood_H0)
    # else:
    #     return np.exp(np.real(log_likelihood_H1 - log_likelihood_H0))


def covariance_equality_t1_gaussian_statistic(ğ—, args=None):
    """ t1 statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
        Outputs:
            * the t1 statistic given the observations in input"""

    (N, K, M) = ğ—.shape

    Sigma_10 = SCM(ğ—.reshape((N, K*M)))
    iSigma_10 = np.linalg.inv(Sigma_10)
    t1 = 0
    for t in range(0, M):
        Sigma_m1 = SCM(ğ—[:, :, t])
        S = (iSigma_10 @ Sigma_m1)
        t1 = t1 + np.trace( S @ S )/M;

    if args is not None:
        if args=='log':
            return np.log(np.real(t1))
        else:
            return np.real(t1)
    return np.real(t1)


def covariance_equality_Wald_gaussian_statistic(ğ—, args=None):
    """ Wald statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
        Outputs:
            * the Wald statistic given the observations in input"""

    (N, K, M) = ğ—.shape
    L = 0;
    O = 0;
    Q = 0;
    Sigma_11 = SCM(ğ—[:, :, 0])
    for m in range(0,M):
        Sigma_m1 = SCM(ğ—[:, :, m])
        iSigma_m1 = np.linalg.inv(Sigma_m1)
        if m != 0:
            S = np.eye(N) - Sigma_11@iSigma_m1
            L = L + K*np.trace(S@S)
            Q = Q + K*(iSigma_m1 - iSigma_m1@Sigma_11@iSigma_m1)
        O = O + K*np.kron(iSigma_m1.T, iSigma_m1)
    
    if args is not None:
        if args=='log':
            return np.real(np.real(L - vec(Q).conj().T @ (np.linalg.inv(O)@vec(Q)))[0,0])
        else:
            return np.real(L - vec(Q).conj().T @ (np.linalg.inv(O)@vec(Q)))[0,0]
    return np.real(L - vec(Q).conj().T @ (np.linalg.inv(O)@vec(Q)))[0,0]


##############################################################################
# Robust Statistics
##############################################################################
def student_t_shape_statistic_d_known(X, Args):
    """ GLRT test for testing a change in the shape of a multivariate
        Student-t distribution when the degree of freefom is known.
        Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = d=degree of freedom and tol, iterMax for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    d, tol, iter_max, scale = Args
    (p, N, T) = ğ—.shape

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = student_t_estimator_covariance_mle(ğ—.reshape((p,T*N)), d, tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    logğ›•_0 = 0
    logğ›•_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = student_t_estimator_covariance_mle(ğ—[:,:,t], d, tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing quadratic terms
        logğ›•_0 =  logğ›•_0 + np.log(d + np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]))
        logğ›•_t = logğ›•_t + np.log(d + np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = (d+p)*np.sum(logğ›•_0)
    log_denominator_quadtratic_terms = (d+p)*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»


def scale_and_shape_equality_robust_statistic_low_rank(ğ—, args):
    """ Low-Rank GLRT test for testing a change in the scale or/and shape of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, rank, scale
        Outputs:
            * the statistic given the observations in input"""


    tol, iter_max, R, scale = args
    (p, N, T) = ğ—.shape

    # Estimating Ïƒ2 
    ğšº = SCM(X.reshape((p,T*N)))
    u, s, vh = np.linalg.svd(ğšº)
    Ïƒ2 = s[R:].mean()

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    ğ›•_0 = 0
    logğ›•_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance_low_rank(ğ—[:,:,t], R, Ïƒ2, tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        ğ›•_0 =  ğ›•_0 + np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]) / T
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(ğ›•_0))
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»


def scale_and_shape_equality_robust_statistic(ğ—, args):
    """ GLRT test for testing a change in the scale or/and shape of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = ğ—.shape

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = tyler_estimator_covariance_matandtext(ğ—, tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    ğ›•_0 = 0
    logğ›•_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance(ğ—[:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        ğ›•_0 =  ğ›•_0 + np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]) / T
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(ğ›•_0))
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»

def shape_equality_robust_statistic(ğ—, args):
    """ GLRT test for testing a change in the shape of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = ğ—.shape

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = tyler_estimator_covariance(ğ—.reshape((p,T*N)), tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    logğ›•_0 = 0
    logğ›•_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance(ğ—[:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        logğ›•_0 =  logğ›•_0 + np.log(np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]))
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = p*np.sum(logğ›•_0)
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»


def scale_equality_robust_statistic(ğ—, args):
    """ GLRT test for testing a change in the scale of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = ğ—.shape

    # Estimating ğšº_t under H0 regime using all the observations
    (ğšº_0, ğ›…, niter) = tyler_estimator_covariance_text(ğ—, tol, iter_max)

    # Some initialisation
    log_numerator_determinant_terms = 0
    log_denominator_determinant_terms = 0
    ğ›•_0 = 0
    logğ›•_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):

        # Estimating ğšº_t under H1 regime
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance(ğ—[:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_numerator_determinant_terms
        log_numerator_determinant_terms = log_numerator_determinant_terms + \
                                        N*np.log(np.abs(np.linalg.det(ğšº_0[:,:,t])))

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        ğ›•_0 =  ğ›•_0 + np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_0[:,:,t])@ğ—[:,:,t]) / T
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(ğ›•_0))
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»

##############################################################################
# Some Functions
##############################################################################
def tyler_estimator_covariance_matandtext(ğ—, tol=0.0001, iter_max=20):
    """ A function that computes the Modified Tyler Fixed Point Estimator for 
    covariance matrix estimation under problem MatAndText.
        Inputs:
            * ğ— = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """

    (p, N, T) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = np.eye(p) # Initialise estimate to identity
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates avalaibe
        Ï„ = 0
        iğšº = np.linalg.inv(ğšº)
        for t in range(0, T):
            Ï„ = Ï„ + np.diagonal(ğ—[:,:,t].conj().T@iğšº@ğ—[:,:,t])

        # Computing expression of the estimator
        ğšº_new = 0
        for t in range(0, T):
            ğ—_bis = ğ—[:,:,t] / np.sqrt(Ï„)
            ğšº_new = ğšº_new + (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing trace constraint: Tr(ğšº) = p
        ğšº_new = p*ğšº_new/np.trace(ğšº_new)

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')

        # Updating ğšº
        ğšº = ğšº_new
        iteration = iteration + 1

    # if iteration == iter_max:
    #     warnings.warn('Recursive algorithm did not converge')

    return (ğšº, Î´, iteration)


def tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol=0.0001, iter_max=20):
    """ A function that computes the Modified Tyler Fixed Point Estimator Low Rank for 
    covariance matrix estimation under problem MatAndText.
        Inputs:
            * ğ— = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * R = Rank
            * Ïƒ2 = noise lvel
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """

    (p, N, T) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—.reshape((p,T*N))) # Initialise estimate to SCM
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates avalaibe
        Ï„ = 0
        iğšº = np.linalg.inv(ğšº)
        for t in range(0, T):
            Ï„ = Ï„ + np.diagonal(ğ—[:,:,t].conj().T@iğšº@ğ—[:,:,t])

        # Computing expression of the estimator
        ğšº_new = 0
        for t in range(0, T):
            ğ—_bis = ğ—[:,:,t] / np.sqrt(Ï„)
            ğšº_new = ğšº_new + (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        u, s, vh = np.linalg.svd(ğšº_new)
        s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
        s_noise = Ïƒ2 * np.ones((p-R,))
        s = np.diag(np.hstack([s_signal, s_noise]))
        ğšº_new = u @ s @ vh

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')

        # Updating ğšº
        ğšº = ğšº_new
        iteration = iteration + 1


    return (ğšº, Î´, iteration)


def tyler_estimator_covariance_text(ğ—, tol=0.0001, iter_max=20):
    """ A function that computes the Modified Tyler Fixed Point Estimator for
    covariance matrix estimation under problem TextGen.
        Inputs:
            * ğ— = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = array of size (p,p,T) to the different estimates
            * ğ›… = the final distance between two iterations for each estimate
            * iteration = number of iterations til convergence """

    (p, N, T) = ğ—.shape
    ğ›… = np.inf*np.ones(T) # Distance between two iterations for each t
    ğšº = np.tile(np.eye(p).reshape(p,p,1), (1,1,T)) # Initialise all estimates to identity
    iteration = 0

    # Recursive algorithm
    while (np.max(ğ›…)>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates avalaibe
        Ï„ = 0
        for t in range(0, T):
            iğšº_t = np.linalg.inv(ğšº[:,:,t])
            Ï„ = Ï„ + np.diagonal(ğ—[:,:,t].conj().T@iğšº_t@ğ—[:,:,t])

        # Computing expression of the estimator
        ğšº_new = np.zeros((p,p,T)).astype(complex)
        for t in range(0, T):
            ğ—_bis = ğ—[:,:,t] / np.sqrt(Ï„)
            ğšº_new[:,:,t] = (T*p/N) * ğ—_bis@ğ—_bis.conj().T

            # Imposing trace constraint: Tr(ğšº) = p
            ğšº_new[:,:,t] = p*ğšº_new[:,:,t]/np.trace(ğšº_new[:,:,t])

            # Condition for stopping
            ğ›…[t] = np.linalg.norm(ğšº_new[:,:,t] - ğšº[:,:,t], 'fro') / \
                     np.linalg.norm(ğšº[:,:,t], 'fro')
        
        # Updating ğšº
        ğšº = ğšº_new
        iteration = iteration + 1

    # if iteration == iter_max:
    #     warnings.warn('Recursive algorithm did not converge')

    return (ğšº, ğ›…, iteration)


def tyler_estimator_covariance_low_rank(ğ—, R, Ïƒ2, tol=0.001, iter_max=20):
    """ A function that computes the Tyler Fixed Point Estimator for covariance matrix estimation
        Inputs:
            * ğ— = a matrix of size p*N with each observation along column dimension
            * tol = tolerance for convergence of estimator
            * R = Rank
            * Ïƒ2 = noise level
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """

    # Initialisation
    (p,N) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—) # Initialise estimate to identity
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and (iteration<iter_max):
        
        # Computing expression of Tyler estimator (with matrix multiplication)
        Ï„ = np.diagonal(ğ—.conj().T@np.linalg.inv(ğšº)@ğ—)
        ğ—_bis = ğ— / np.sqrt(Ï„)
        ğšº_new = (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        u, s, vh = np.linalg.svd(ğšº_new)
        s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
        s_noise = Ïƒ2 * np.ones((p-R,))
        s = np.diag(np.hstack([s_signal, s_noise]))
        ğšº_new = u @ s @ vh

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')
        iteration = iteration + 1

        # Updating ğšº
        ğšº = ğšº_new

    return (ğšº, Î´, iteration)